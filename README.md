# HealthCareâ€‘GPT

*A lightweight, endâ€‘toâ€‘end framework for fineâ€‘tuning a GPTâ€‘2â€‘style model on healthcare data â€” fully runnable on a single PC with optional Docker.*


---

## âœ¨ Key Features

* **Localâ€‘First**: Train and run on 8â€‘16â€¯GB RAM machines; leverages GPU if present, but works on CPU.
* **Dualâ€‘Task Fineâ€‘Tuning**: Combines medical domain adaptation + highâ€‘risk patient classification.
* **Continuous Learning**: Drop new `.csv`, `.md`, `.txt` into `HealthCareData/` and let agents retrain automatically.
* **Selfâ€‘Documenting**: Agentic pipeline logs changelogs & summaries in `docs/changelog/` and `docs/summaries/`.
* **Oneâ€‘Command Docker**: `docker run -p 5000:5000 healthcare-gpt` starts a REST/Gradio service instantly.

---

## ðŸ“‚ Repository Layout

```
HealthCare-GPT/
â”œâ”€â”€ nanoGPT/              # Core GPT model + training loop (fork of Karpathy's)
â”œâ”€â”€ HealthCareData/       # Raw data (CSV patients, research docs)
â”œâ”€â”€ models/               # Pretrained & fineâ€‘tuned checkpoints
â”œâ”€â”€ scripts/              # CLI helpers (preprocess, finetune, inference, serve)
â”œâ”€â”€ config/               # YAML experiment configs
â”œâ”€â”€ docs/                 # Plans, changelogs, summaries, assets
â”œâ”€â”€ Dockerfile            # Container build
â”œâ”€â”€ requirements.txt      # Python deps
â””â”€â”€ README.md             # (you are here)
```

*See `docs/plans/claude.md` for the full agent design.*

---

## ðŸš€ QuickÂ Start

### 1. Clone & install

```bash
git clone https://github.com/yourâ€‘org/HealthCareâ€‘GPT.git
cd HealthCareâ€‘GPT
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

> **Tip**: GPU users should install the CUDAâ€‘matched PyTorch wheel.

### 2. Prepare data

Place patient CSVs in `HealthCareData/patient_data/` and research docs in `HealthCareData/research_docs/`, then run:

```bash
python scripts/preprocess.py \
  --config config/config.yaml
```

### 3. Fineâ€‘tune

```bash
python scripts/finetune.py \
  --config config/config.yaml
```

Checkpoints land in `models/finetuned/` and metrics in `docs/summaries/`.

### 4. Inference (CLI)

```bash
python scripts/inference.py --patient_csv demo.csv
```

Or interactive mode:

```bash
python scripts/inference.py
> Patient: Age 70, BloodPressure 160/110, Diabetes Yesâ€¦ Risk:
HighRisk
```

### 5. Serve as API/UI

```bash
python scripts/serve.py  # Flask or Gradio (configurable)
```

Open [http://localhost:5000](http://localhost:5000) for the web UI.

---

## ðŸ³ Run in Docker

```bash
docker build -t healthcare-gpt .
docker run --gpus all -p 5000:5000 -v $PWD/HealthCareData:/app/HealthCareData healthcare-gpt
```

The container starts the Gradio interface by default; override `CMD` for training tasks.

---

## âš™ï¸ Configuration

All hyperâ€‘parameters live in `config/`:

```yaml
model:
  n_layers: 12
  n_heads: 12
  d_emb: 768
train:
  batch_size: 8
  epochs: 5
  learning_rate: 3eâ€‘5
```

Create alternate YAMLs (e.g., `config_small.yaml`) and pass with `--config` to experiment.

---

## ðŸ¤– Agentic Workflow

1. **DataÂ Curator** watches `HealthCareData/`, triggers preprocessing.
2. **Trainer** runs staged fineâ€‘tuning, logging to W\&B.
3. **Evaluator** computes accuracy/F1 on heldâ€‘out CSV.
4. **Deployer** rebuilds Docker image & smokeâ€‘tests inference.
5. **Docâ€‘Bot** writes `docs/changelog/YYYYMMDD__*.md` & daily summaries.

See the [Claude Plan](docs/plans/claude.md) for sequence diagrams and future extensions.

---

## ðŸ“Š Example Results

| Metric   | Validation |
| -------- | ---------- |
| Accuracy | 0.91       |
| F1â€‘score | 0.89       |

*(Numbers shown are from the sample dataset; your mileage may vary.)*

---

## ðŸ›  Development & Contribution

Pull requests are welcome! Please:

1. Fork â†’ feature branch.
2. `pre-commit run --all-files` to satisfy linting.
3. Add/maintain tests where relevant.
4. Update `docs/changelog/` via `make docs` or describe changes in PR.

---

## ðŸ“„ License

MIT â€” see `LICENSE` for details.

> **Citation**: If you use HealthCareâ€‘GPT in academic work, please cite the repository URL and original nanoGPT paper.
